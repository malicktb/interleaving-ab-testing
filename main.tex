\documentclass[11pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{booktabs}

% Margins
\geometry{
  top=1in,
  bottom=1in,
  left=1in,
  right=1in
}

% Title Data
\title{\textbf{Accelerating Online Ranker Evaluation via Multi-Dueling Bandits}\\
\large A Semi-Synthetic Simulation Study using Yahoo! LTR Data}
\author{Research Proposal}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
In large-scale recommendation systems, identifying the optimal ranking algorithm from dozens of candidates is a scalability bottleneck. Traditional A/B testing scales poorly ($O(K)$ time complexity) and incurs high cumulative regret. This research proposes a \textbf{Multi-Dueling Bandit (MDB)} framework as a high-efficiency ``Candidate Screening Layer'' to rapidly filter rankers before final A/B validation. By utilizing \textbf{Team Draft Multileaving} and a \textbf{Semi-Synthetic Simulation} on the \textbf{Yahoo! Learning-to-Rank (LTR)} dataset, we aim to demonstrate that MDBs can identify the optimal ranker with significantly fewer user impressions than sequential testing. Crucially, this study includes a ``stress test'' involving \textbf{Non-Stationary (Learning) Agents} (LinUCB) and explicitly analyzes the trade-off between Sample Efficiency (user impressions) and Inference Cost (computational overhead).
\end{abstract}

\section{Problem Statement}
The current industry standard for evaluating ranking models is the ``Bucket Test'' (A/B Test). While effective for final business validation, it presents three critical issues during the initial model development phase:

\begin{itemize}
    \item \textbf{Sample Inefficiency:} Comparing $K$ models pairwise requires massive traffic or long durations. Testing $20+$ models sequentially is often infeasible as complexity scales with $O(K)$ or $O(K^2)$.
    \item \textbf{User Regret:} Users in ``losing'' buckets are exposed to inferior rankings for the entire duration of the test, degrading the user experience.
    \item \textbf{Data Inefficiency:} Standard A/B tests ignore the rich comparative feedback available when multiple rankers contribute to a single result page (multileaving).
\end{itemize}

\textbf{Proposed Solution:} We position the MDB framework not as a replacement for final business metric validation, but as an \textbf{offline/online pre-selection funnel}. The goal is to reduce a pool of $K=20$ candidate models down to the top 2 ``Elite'' models using minimal user interactions.

\section{Methodology: The Simulation Framework}
To avoid the counterfactual bias inherent in replaying historical logs (where unshown items have unknown relevance), we will employ a \textbf{Semi-Synthetic Simulation} using ground-truth relevance data.

\subsection{Dataset: Yahoo! Learning-to-Rank Challenge}
We utilize the Yahoo LTR Challenge dataset (Set 1), which provides approximately 30,000 queries with human-labeled relevance grades (0â€“4).
\begin{itemize}
    \item \textbf{The Oracle & True Regret:} Because we possess the true labels, we can construct an Oracle Ranker. To ensure reproducibility, we employ \textbf{Deterministic Tie-Breaking}: documents are sorted first by Relevance Grade (Descending) and then by Document ID (Ascending). This allows us to calculate \emph{True Regret} (the difference in NDCG between the MDB's selection and the Oracle) without noise.
\end{itemize}

\subsection{Probabilistic User Simulator (Robustness Checks)}
To ensure the findings are not artifacts of a specific user behavior assumption, we will evaluate under three distinct environments:

\begin{enumerate}
    \item \textbf{Standard Position-Based Model (PBM):} The user has a higher probability of observing top-ranked items (decay curve). Clicks are probabilistic based on relevance grade.
    \item \textbf{The ``Perfect Cascade'' (Sparse Feedback):} The user scans from top to bottom and clicks \emph{only} the first relevant item (Grade $\ge 3$), then stops. This tests the algorithm under sparse signal conditions.
    \item \textbf{Noisy/Irrational User:} A PBM user with a small noise parameter ($\nu=0.1$), introducing a 10\% chance of clicking a random document regardless of relevance.
\end{enumerate}

\section{The Algorithm: Multi-Dueling Bandit (MDB)}

\subsection{Team Draft Multileaving (TDM)}
For every query, the system selects a subset of rankers (arms) and interleaves their results using the Team Draft method \cite{schuth2014multileaved}. Clicks are attributed to the contributing ranker to update a Pairwise Win Matrix $C_{i,j}$ (wins of ranker $i$ over $j$).

\subsection{Selection Logic: UCB with ``Grace Period''}
We use an Upper Confidence Bound (UCB) approach \cite{brost2016multi} to maintain two sets:
\begin{itemize}
    \item \textbf{Set $E$ (Elite):} Potential Condorcet winners.
    \item \textbf{Set $F$ (Contenders):} Rankers not yet statistically ruled out.
\end{itemize}

\textbf{The Grace Period ($N_{min}$):} A standard UCB algorithm might eliminate a learning agent (which starts with poor performance) too early. We implement a tunable grace period (e.g., $N_{min} = 500$ queries) during which no ranker can be eliminated from Set $F$, ensuring non-stationary agents have time to learn.

\section{The Agent Pool (The Arms)}
We will evaluate a diverse pool of $K=20$ agents:
\begin{enumerate}
    \item \textbf{Static Baselines:} Random Ranker (Lower Bound), BM25 (Standard IR).
    \item \textbf{Supervised Models:} XGBoost (trained on 10\% of data), LambdaMART.
    \item \textbf{The Stress Test (Non-Stationary Agent): LinUCB.}
\end{enumerate}

\subsection{Feasibility Fix for LinUCB}
Running a Contextual Bandit (LinUCB \cite{li2010contextual}) with the raw Yahoo dataset (700+ features) is computationally prohibitive for a simulation, as it requires inverting a $700 \times 700$ matrix at every step.
\begin{itemize}
    \item \textbf{Solution:} We will apply \textbf{Principal Component Analysis (PCA)} to the Yahoo dataset offline (fit only on the training partition), reducing the feature space to the top 20 components.
    \item \textbf{Benefit:} This ensures the learning agent is computationally feasible and can converge within the simulation horizon.
\end{itemize}

\section{Evaluation Plan \& Metrics}

\subsection{Key Metrics}
\begin{itemize}
    \item \textbf{Sample Complexity (Speed):} Number of queries required to identify the best ranker with 95\% confidence.
    \item \textbf{True Cumulative Regret:} The total loss in NDCG compared to the Oracle.
    \item \textbf{Total Inference Cost (The ``Hidden'' Cost):} Defined as $\text{Queries} \times \text{Rankers Evaluated Per Query}$. MDB uses $K$ inferences per query; A/B uses 1. We will explicitly quantify this trade-off to demonstrate that CPU cost is the price paid for data efficiency.
\end{itemize}

\subsection{Baselines}
We compare the MDB framework against:
\begin{enumerate}
    \item \textbf{Simulated Sequential A/B Testing:} Round-robin pairwise comparisons.
    \item \textbf{Single-Arm Contextual Bandit:} A standard bandit (e.g., Thompson Sampling) that selects only \emph{one} ranker to show per query, ignoring multileaving.
\end{enumerate}

\section{Conclusion}
We expect to demonstrate that MDB reduces the \textbf{Time-to-Decision} by an order of magnitude compared to A/B testing. By incorporating robust sensitivity analysis and realistic constraints (PCA for LinUCB), this study provides a rigorous blueprint for implementing bandit-based candidate screening in production systems.

\bibliographystyle{plain}
\bibliography{references}

\end{document}