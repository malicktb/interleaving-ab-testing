\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{xcolor}

\geometry{margin=1in}

\title{\textbf{Accelerating Online Ranker Evaluation: \\ A Multi-Dueling Bandit Framework}}
\author{Research Proposal Summary}
\date{}

\begin{document}

\maketitle

\begin{abstract}
In large-scale recommendation systems, the ability to rapidly evaluate and select the best ranking algorithm is a critical competitive advantage. Traditional A/B testing methods scale poorly ($O(K^2)$) and incur high cumulative regret. This proposal outlines a \textbf{Multi-Dueling Bandit (MDB)} framework that leverages team-draft multileaving and adaptive sampling to evaluate multiple rankers simultaneously. By treating ranker evaluation as an online learning problem, the framework reduces the time-to-convergence and improves sample efficiency. We further propose an enhanced implementation strategy that includes Gradient Boosted Decision Trees (XGBoost) and Thompson Sampling to ensure robustness across linear and non-linear feature interactions.
\end{abstract}

\section{Background and Problem Overview}

In domains such as e-commerce storefronts and news feeds, the display order of items (re-ranking) fundamentally impacts user engagement and revenue. Finding the optimal sorting algorithm is challenging when dozens of candidate rankers exist. Traditional online evaluation methods, such as A/B tests or standard dueling bandits, compare only two rankers at a time. This approach presents three major systemic issues:

\begin{itemize}
    \item \textbf{Scalability Bottleneck:} Pairwise comparisons scale poorly. If there are $K$ candidate rankers, the number of A/B tests required grows on the order of $K^2$. Testing models sequentially is impractical, slowing down the deployment of improvements.
    
    \item \textbf{High Cumulative Regret:} During a test, a significant portion of users is exposed to a suboptimal ranking (the "losing" algorithm). In a re-ranking scenario, where slight mistakes bury relevant content, the system incurs regret by failing to show the best ranking to every user until the test concludes.
    
    \item \textbf{Inefficient Use of Feedback:} Standard tests treat a user click as a binary win/loss for one ranker, ignoring the rich contextual data available. When two rankers are interleaved, the position of items and the non-clicks on other items provide valuable signals. Standard A/B testing wastes the data generated by non-chosen items, requiring large sample sizes to distinguish between closely performing algorithms.
\end{itemize}

\textbf{Context:} These issues are prevalent in datasets such as the \textit{Alibaba Personalized Re-Ranking} dataset (e-commerce) and the \textit{Microsoft MIND} dataset (news recommendation). Both involve presenting a slate of items where efficient evaluation is key to performance.

\section{Proposed Solution: Multi-Dueling Bandit Framework}

We propose replacing sequential pairwise evaluations with a \textbf{Multi-Dueling Bandit (MDB)} framework capable of evaluating multiple rankers simultaneously. The core concept views the presentation of a mixed list of items as an \textit{adaptive Monte Carlo sampling process}.

\subsection{Key Concept: Multileaving as Monte Carlo}
By using \textbf{Team Draft Multileaving}, the system creates a displayed list by mixing top results from several ranking algorithms. This list acts as a sample from an underlying probability distribution over rankers. A user's click on an item is a stochastic reward credited to the ranker that recommended it. By adaptively adjusting the sampling probability based on these rewards, the system performs a Monte Carlo simulation to estimate ranker quality, "zeroing in" on the best ranker significantly faster than separate tests.

\subsection{Bandit Selection Algorithm (The "Store Manager")}
The implementation extends the algorithm by Brost et al. (2016). The bandit acts as a "Store Manager," dynamically allocating shelf space (impressions) to algorithms based on past performance using Upper Confidence Bounds (UCB).

For every pair of rankers $(i, j)$, the system maintains a win-rate estimate $\hat{p}_{ij}$ and calculates two bounds:
\begin{itemize}
    \item \textbf{Narrow UCB ($u_{ij}$):} A high-confidence, conservative estimate.
    \item \textbf{Wide UCB ($v_{ij}$):} An optimistic estimate with a larger confidence buffer.
\end{itemize}
\[
u_{ij}(t) = \hat{p}_{ij} + \sqrt{\frac{\alpha \ln t}{n}}, \quad v_{ij}(t) = \hat{p}_{ij} + \sqrt{\frac{\beta \alpha \ln t}{n}} \quad (\text{where } \beta \ge 1)
\]
Based on these bounds, rankers are classified into two sets:
\begin{enumerate}
    \item \textbf{Proven Winners (Set $E$):} Rankers that are statistically likely to beat all others (Condorcet winners).
    \item \textbf{Contenders (Set $F$):} Rankers that are not proven losers. This set includes high-uncertainty arms that need more exploration.
\end{enumerate}

\textbf{Adaptive Strategy:} If $|E| = 1$, the system exploits the winner. If uncertainty exists ($|E| \neq 1$), the system runs a "Battle Royale" among Set $F$, allocating impressions to all promising rankers simultaneously. This minimizes regret by quickly discarding inferior models.

\section{Methodology and Implementation}

To validate the framework, we utilize an \textbf{Offline Replay Simulation} using the \textit{Alibaba Personalized Re-Ranking Dataset} (6.7 million sessions). This method allows us to "replay" historical sessions by re-ranking the 30 candidate items logged in the dataset and checking if the user's actual clicked item appears in the bandit's top slots.

\subsection{Expanded Pool of Ranking Agents}
A critique of standard bandit evaluations is the reliance on weak baselines. To ensure the MDB is robust for real-world deployment, this implementation integrates a diverse set of "Merchandisers" (ranking agents), ranging from simple heuristics to state-of-the-art machine learning models.

\subsubsection{Baseline Agents}
\begin{itemize}
    \item \textbf{Popularity Bot:} Ranks items by global Click-Through Rate (CTR) using a smoothed Dirichlet prior.
    \item \textbf{Random Bot (Control):} Produces a random ordering. This serves as a sanity check; the bandit must identify and discard this agent rapidly.
\end{itemize}

\subsubsection{Linear and Discriminant Models}
\begin{itemize}
    \item \textbf{Linear Bot (Logistic Regression):} Uses a linear model on 27 feature dimensions (item numeric/categorical features + user context) to predict click probability.
    \item \textbf{LDA/QDA Agents (Enhanced):} We augment the linear pool with \textbf{Linear Discriminant Analysis (LDA)} and \textbf{Quadratic Discriminant Analysis (QDA)}. LDA provides a robust alternative to logistic regression by modeling class densities, while QDA captures quadratic feature interactions. These models serve as interpretable benchmarks that may outperform logistic regression if feature distributions are approximately Gaussian.
\end{itemize}

\subsubsection{Tree-Based Ensemble Models (Key Addition)}
Current industry standards for CTR prediction often rely on tree ensembles. To address the gap in modeling non-linear feature interactions:
\begin{itemize}
    \item \textbf{XGBoost/Gradient Boosting Agent:} We implement a Gradient Boosted Decision Tree ranker. Prior research indicates boosted trees (e.g., XGBoost, LightGBM) frequently outperform Logistic Regression and SVMs on tabular data by automatically capturing complex feature interactions and non-linearities.
    \item \textbf{Random Forest Agent:} A variance-reducing ensemble model that provides stability and feature importance interpretability. 
\end{itemize}
\textit{Hypothesis:} The bandit should rapidly favor the Tree-Based agents over the Linear Bot due to their superior capacity to model the decision boundary in high-dimensional space.

\subsubsection{Content-Based and Neural Models}
\begin{itemize}
    \item \textbf{Enhanced k-NN Bot:} A non-parametric approach. Beyond a global centroid, this agent calculates a \textit{per-segment ideal vector} or uses a voting mechanism among the $k$ nearest historical successful items. This captures geometric similarity in feature space.
    \item \textbf{MLP and Neural Extensions:} A Multi-Layer Perceptron (feed-forward network) to capture deep feature interactions. Future iterations will include \textbf{Two-Tower architectures} (learning user and item embeddings separately) to better handle the sparse categorical data typical in e-commerce and news (MIND dataset).
\end{itemize}

\subsection{Algorithmic Refinements: Thompson Sampling}
While the primary implementation uses UCB (deterministic optimism), we acknowledge that UCB can be conservative. We will integrate \textbf{Thompson Sampling (TS)} as an alternative exploration strategy. TS samples ranker parameters from a posterior distribution (e.g., Beta distribution for win rates). 
\begin{itemize}
    \item \textbf{Rationale:} TS handles the exploration-exploitation trade-off more naturally in noisy reward environments (like click data) and prevents the algorithm from getting stuck in local optima.
    \item \textbf{Metric:} We will compare the convergence speed of UCB-MDB versus TS-MDB.
\end{itemize}

\subsection{Evaluation Metrics}
The framework is evaluated against four key objectives:
\begin{enumerate}
    \item \textbf{Scalability:} We measure the number of iterations required to identify the best ranker as $K$ increases. We expect convergence time to scale linearly $\sim O(K)$.
    \item \textbf{Regret Reduction Ratio:} Comparing the cumulative clicks lost by the MDB strategy versus a naive Round-Robin strategy.
    \item \textbf{Sample Efficiency:} Calculated as $(\text{Samples required by pairwise}) / (\text{Samples used by MDB})$. We expect a ratio significantly $>1$.
    \item \textbf{Overall CTR Improvement:} The aggregate click-through rate of the system during the learning phase. A successful bandit improves user experience (higher CTR) even while experimenting.
\end{enumerate}

\section{Conclusion}
This enhanced Multi-Dueling Bandit framework provides a scalable, safe method for online ranker evaluation. By integrating robust agents (XGBoost, LDA) and advanced exploration strategies (Thompson Sampling), the system is designed not just for academic simulation but for realistic, high-velocity industrial deployment.

\end{document}